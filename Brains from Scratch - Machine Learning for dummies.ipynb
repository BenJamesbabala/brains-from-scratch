{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brains from Scratch\n",
    "> Machine Learning for dummies - *Achilles Rasquinha*\n",
    "\n",
    "![](./img/cover.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q. What is this page all about?\n",
    "This is a one-page comprehensive publish that aims to help you learn and understand what **Machine Learning** is and the workings behind *\"how a machine thinks\"*, all right from scratch. We'll not only be building *learning/prediction models* together but also understand and realize the nuts-and-bolts that make it work.\n",
    "\n",
    "This also means that there is going to be exhaustive mathematics (linear algebra, calculus, etc.) throughout the way but do not be intimidated already if you aren't coming from a mathematical background. This publish also aims to teach you the mathematics behind learning models in the most simplest of terms present in the English language. I'll be using the keyword **ST** (for *in simpler terms*) to denote something hard to comprehend in the most simplest of terms. For instance,\n",
    "\n",
    "$$\\frac{dy}{dx}$$\n",
    "\n",
    "**ST**\n",
    "\n",
    "The notation above denotes an operation $\\frac{d}{dx}$ on a variable $y$ to realize *\"at what rate does $y$ change with respect to $x$\".* Assume the variable $y$ to be the anger temperament of your boss and $x$ to be your skills to mollify an individual well. If it were to be a perfect linear relation, the above factor would denote no appreciable rate of change whatsoever. \n",
    "\n",
    "***WARNING:*** Do not consider such \"simple english\" to solve various mathematical problems. Mathematics per se has a different way of expressing the world and is binded by its rules and grammar. Mathematics provides you reasoning that is universally accepted and is never ambiguous. By all means, you should never merge the two domains for problem solving. We'll try incorporating the basics behind the mathematics that is going to be used throughout this publish.\n",
    "\n",
    "We then remain true to our title - **Machine Learning for dummies**\n",
    "\n",
    "***PS:*** Do not be disheartened by the word dummies. We aim to present this publish even for those who aren't able make sense of all that scientific textbook jargon.\n",
    "\n",
    "This publish will be regularly updated over the course of time, attempting to learn and build new models and solve problems. You can keep track of these updates in the [What's new?](#What's-new?) section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $ whoami"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/author.png\" style=\"float: left; padding-left: 10px; padding-right: 10px;\">\n",
    "Hi, I'm Achilles Rasquinha.\n",
    "\n",
    "I'm currently an undergraduate at the Department of Computer Engineering, University of Mumbai and expected to graduate in the year 2017.\n",
    "\n",
    "You can contact me in the following ways, I'd be happy to help!\n",
    "\n",
    "Type    |Description                \n",
    "--------|---------------------------\n",
    "Email   |achillesrasquinha@gmail.com\n",
    "Facebook|[www.facebook.com/rasquinhaachilles](https://www.facebook.com/rasquinhaachilles)\n",
    "LinkedIn|[www.linkedin.com/in/achillesrasquinha](https://www.linkedin.com/in/achillesrasquinha)\n",
    "GitHub  |[www.github.com/achillesrasquinha](https://www.github.com/achillesrasquinha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgements\n",
    "I'd like to thank my Professor - Mrs. Anuradha Srinivas for introducing me first into the field of Machine Learning and also helping me engage in solving a wide range of problems within the domain. I'd also like to thank my Professor - Mrs. Vincy Joseph for being a supportive guide and mentor as I work on my Bachelor's thesis on *Image Context Analysis and Contextual Caption Generation Using RNNs/LSTMs*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without further ado, let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "**Choice of Programming Language**\n",
    "\n",
    "![](https://imgs.xkcd.com/comics/python.png)\n",
    "\n",
    "Image Source: [xkcd](http://xkcd.com/)\n",
    "\n",
    "Solving complex mathematical problems require some state-of-the-art tools, especially when it comes to the field of Machine Learning. We'll be implementing our models and perform our analysis using one such tool (to be precise, a programming language) - **Python**.\n",
    "\n",
    "But, why Python?\n",
    "\n",
    "Simply because of its *\"english-like\"* code structure that attempts to somewhat make linguistic sense to people who may have never written a single line of code. Moreover, Python comes with a wide range of scientific computing libraries (for linear algebra computations, calculus, graph visualizations, etc.) which we'll be using throughout our way.\n",
    "\n",
    "Finally (and this is my favourite), we've chosen Python because of its high rate of productivity. Python has a high rate of productivity by ***minimizing the number of lines of code*** (in Computer Science, we call this unit - LOC) to write some logic, in comparision to other programming languages such as C++, Java, etc. <sup>[[1]](#References)</sup> Performance wise, the latter perform way better than Python (in terms of time and space complexity). We're however not keen in optimizing our code (by choice of language) but with the said limitations, we'll try out best to.\n",
    "Also, Python is a general-purpose high programming language.\n",
    "\n",
    "So Python does seem to fit the jig-saw."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's new?\n",
    "* October 28th, 2016\n",
    "    - Introduction to Genetic Algorithms\n",
    "    - Kaggling\n",
    "    - Case Study 1: Performance Analysis of k-NN, ANN and SVM Classifiers for Cotton Germplasm Classification\n",
    "* October 15th, 2016\n",
    "    - Introduction to Artificial Neural Networks.\n",
    "    - Implementation of various Activation Functions.\n",
    "    - Implementation of `class ANN`.\n",
    "    - Introduction to Recurrent Neural Networks.\n",
    "* October 12th, 2016\n",
    "    - Introduction to Gradient Descent Optimization.\n",
    "* October 11th, 2016\n",
    "    - Published \"Brains from Scratch - Machine Learning for dummies\".\n",
    "    - Introduction to Machine Learning.\n",
    "    - Types of Learning, Introudced Supervised Learning and its problem types.\n",
    "    - Introduction to our first Machine Learning model - Linear Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index\n",
    "1. [Introduction](#1.-Introduction)\n",
    "    1. [What is Machine Learning?](#1.A.-What-is-Machine-Learning?)\n",
    "-  [Types of Learning](#2.-Types-of-Learning)\n",
    "    1. [Supervised Learning](#2.A.-Supervised-Learning)\n",
    "        1. [Regression](#2.A.a.-Regression)\n",
    "            1. [Linear Regression](#2.A.a.i.-Linear-Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mind Map\n",
    "In order to build a learning path for ourselves, I've created a mind map that can be viewed over [here](https://www.mindmeister.com/786639893/machine-learning)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "### 1.A. What is Machine Learning?\n",
    "![Source www.dilbert.com](http://i.stack.imgur.com/k2eEh.gif)\n",
    "\n",
    "Machine Learning is when **\"a machine turns a statistician\"**. Machine Learning is when a computer observes you while you clean your house (the way you hold your mop, the way you sway it over the floor) and mimicks (well, almost) the steps you performed while you cleaned. Not just that, it may even outperform in terms of tidiness or maybe perform novel ways of cleaning that you weren't even aware of. Machine Learning is when a computer observes you drive (the way you percieve roads) and later attempts to drive a car all by itself (we call it today - Autonomous Driving, a subject that had its seed planted in the 1980s) <sup>[[2]](#References)</sup>\n",
    "\n",
    "Machine Learning is when a machine learns to humanly walk after observing a human walking; **it is also the field when the machine may jaywalk after observing a group of people jaywalking**. This means that we're never expecting a *global truth* or *moral code* from a machine but rather what *truth* and *beliefs* we wish it must follow. At times, the machine may itself find plausible *truths* that is beyond the capabilities of individual observance and thinking (What does it mean to be human? How was the universe formed? etc.)\n",
    "\n",
    "**A machine may be *trained* to even be a sexist**. <sup>[[3]](#References)</sup>\n",
    "\n",
    "Here's what Microsoft AI chatbot @TayandYou had to say about *feminism* after recieveing *learning feedbacks* from Twitter trolls.\n",
    "> *I fucking hate feminists and they should all die and burn in hell.*\n",
    "\n",
    "***NOTE:*** I do not endorse any kind of Microsoft products.\n",
    "\n",
    "So yes, machines are beautiful and maybe even dangerous. But so are humans. The fear that these machines might someday take over the world (Stephen Hawking thinks it might) <sup>[[4]](#References)</sup> is no less compared to that of the fear for a next human genocide, planned by humans themselves.\n",
    "\n",
    "Machines and mathematical models are a reflection of us and the world we live in. So the fear for machines exhibiting immoral behaviour merely highlights the flaws we humans exhibit. Our goal is however, to build machines that truly exhibit some desired behaviour and that is universally accepted.\n",
    "\n",
    "#### Q. \"Hey! All this philosophy is going nowhere. What exactly is machine learning?\" \n",
    "In simple terms, Machine Learning is mapping a variable $x$ (or a set of them) to a variable $y$ (or a set of them) based on its observations of many $x$'s or many $y$'s. You could either direct the machine to move towards the desired $y$ or may leave it alone to discover.\n",
    "\n",
    "To help you understand better, consider each instance from a data set to be one of the many opinions of the members of House of Commons whereas the learning model is the House's Speaker for that session. I consider John Bercow (the current Speaker for the House of Commons) as a fine example. \n",
    "\n",
    "***NOTE:*** I ain't British. One doesn't necessarily need to be British to enjoy the hilarity that arises during Prime Minister's Questions.\n",
    "\n",
    "![](http://66.media.tumblr.com/70e9a137a800ced3d5fd0f8fec20f399/tumblr_nqf83qWw2x1uwvf4go1_1280.jpg)\n",
    "\n",
    "Each member has his or her own say. Some members have a collective say and some may have an exclusive opinion. Members of the Parliament aim to rant their best over a subject in order to influence members of the House (our data set) as well as the Speaker. Based on how effective each member's opinions are relates to the Speaker being influenced by their opinions. One member's opinion may pull a Speaker's decision on one given direction while the other towards another. A collective analysis (either inferencing from some *general truth* or *his/her observance so far*) helps the Speaker of the House to profess his or her conclusions.\n",
    " \n",
    "Machine Learning models are no different from the Speaker of the House. Like the Speaker, a learning model does not caste its own vote and is nothing without a given say (a data set). It is however influenced by the very nature of data that pulls it towards different directions. Like the House of Commons filled with effective and mundane opinions, we conclude that it is in one's best interest for a goal decision if each of the opinons are extremely effective and not merely how many opinions were put forward during the convention. (In the House of Commons, this seems to be the extreme opposite).\n",
    "\n",
    "By this we mean to say that having a large data set does not necessarily yeild a well-learned model but rather a qualitative data set along with the choices of effective $x$'s and $y$'s is what helps a machine make great conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Types of Learning\n",
    "We categorize learning into two kinds\n",
    "### 2.A Supervised Learning\n",
    "As the word suggests, supervised learning attempts to comapre the expected $y$ and the $y$ it has generated for a given $x$, thereby modifying its learning unit such that it comes closer towards the expected output the next time. In simpler terms, a supervised learning algorithm is provided ***what is expected from the model***.\n",
    "\n",
    "#### Traning and Testing Sets\n",
    "A training set is nothing but a portion of a data set we're going to work on and feed into the model. It's never advisable to feed the entire data set into our learning model which is why there exists yet another set called as the testing set that is used to observe whether our learning model predicts well or not.\n",
    "\n",
    "We known that the cardinality (number of elements) of a set (a collection of non-repititive elements) can be given by\n",
    "$$|S| = n$$\n",
    "where $S$ is a set and $n$ is its cardinality.\n",
    "\n",
    "We define a given training set with the notation $Tr$ and its cardinality $|Tr| = u$. Similarly, we define the notation $Te$ and its cardinality $|Te| = v$ for a testing set.\n",
    "\n",
    "We'll be dealing almost always with a multivariate system, i.e. many variables affect the learning unit of our model. Hence,\n",
    "We denote $x_{j}$ to be our input $j$ in our input vector $X$ of length $m$ and $y_{k}$ to be our output $k$ in our output vector $Y$ of length $n$\n",
    "\n",
    "To denote a sample from the training or testing set we define the notation as,\n",
    "$$S(\\{x_{j}^{(i)}\\}, \\{y_{k}^{(i)}\\})$$\n",
    "where S could be $Tr$ or $Te$ and the tuple $(\\{x_{j}^{(i)}\\}, \\{y_{k}^{(i)}\\})$ is nothing but the $i_{th}$ sample of set $S$ denoting an input vector $\\{x_{j}^{(i)}\\}$ and its corresponding output vector $\\{y_{k}^{(i)}\\}$. $j$ and $k$ denote our selected attributes from the input and output vectors respectively.\n",
    "\n",
    "#### Modelling\n",
    "Our model aims to predict output(s) $y$ based on some input(s) $x$. We define this function mapping as\n",
    "\n",
    "$$\\{y_{k}^{(i)}\\} = predict_{\\theta}(\\{x_{j}^{(i)}\\})$$\n",
    "\n",
    "Our prediction model depends not only on the input vector $X$ but also a set of parameters $\\theta = \\{{\\theta_{p}}\\}$ where $p$ ranges from 0 to $o$, $o$ being the cardinality of the parameter vector $\\theta$, i.e. the number of parameters affecting the function $predict$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from abc import ABCMeta, abstractmethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Model(metaclass = ABCMeta):\n",
    "    @abstractmethod\n",
    "    def predict(self, features, parameters):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could further classify supervised learning into two kinds of problems.\n",
    "\n",
    "#### 2.A.a. Regression\n",
    "Let's consider a data set to work on in order to understand what Regression is all about. `scikit-learn` (a machine learning library in Python) provides many toy data sets and we'll consider one of them - the **Boston House Pricing** data set provided under `load_boston()`. University of California, Irvine provides a wide range of open data set repositories to work on, the Housing Prices being one. <sup>[[5]](#References)</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "\n",
    "boston  = load_boston()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize our data set considering the relationship between *average number of rooms per dwelling* and the corresponding house prices. We'll be using `matplotlib` for graph visualizations and `plotly` for interactive plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import plotly.plotly as py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize our data set using a simple scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~achillesrasquinha/16.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npoints = 20                           # number of points to observe\n",
    "X       = boston.data[:,5][:npoints]  # average number of rooms per dwelling\n",
    "y       = boston.target[:npoints]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(X, y)\n",
    "\n",
    "ax.set_title('Boston House Pricing')\n",
    "\n",
    "ax.set_xlabel('Avg. number of rooms per dwelling')\n",
    "ax.set_ylabel('Housing value')\n",
    "\n",
    "py.iplot_mpl(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the plot seems to be dispersed. We can somewhat visualize that there is an increase in the housing value with an increase of average number of rooms per dwelling for the limited data that we've observed. If we were to estimate a given value for $x$ would result in a unique value (real or whole) for $y$. This is known as a **regression** problem, i.e. the function which attempts to estimate for a given value of $x$ is a continous function. We could then somewhat fit a line or draw a curve that passes through almost all the points on the scatter plot and aim to estimate approximately well for any given input $x$.\n",
    "\n",
    "Machine Learning is all about building that model of estimation/prediction. The example states to build a model that can **draw a line** (a polynomial equation to be precise) across these points in order to maximize the *closeness* with $y$ for any given $x$. Such a model is called as a Linear Regression model which we'll discuss further. (In the case above, we have a single variable $x$ and hence is a univariate problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.A.a.i. Linear Regression\n",
    "\n",
    "In its simplest sense, a Linear Regressor attempts to find the best fit line that can be drawn from a given set of features (or inputs) with respect to a given set of labels (or outputs).\n",
    "\n",
    "As mentioned, a Supervised Learning algorithms maps a feature $x$ to $y$ through a function $predict$. We can then define a $predict$ function for a Multivariate Linear Regressor as\n",
    "$$predict_{\\theta}(x_{j}^{(i)}) = \\sum_{j = 0}^{m}\\theta_{j}x_{j}^{(i)}$$\n",
    "where $j = [0, m]$ and $x_{0}^{(i)} = 1$\n",
    "\n",
    "In a vector notation we can then denote the above equation as\n",
    "$$predict_{\\theta}(X^{(i)}) = \\theta^{T}X^{(i)}$$\n",
    "\n",
    "where $X$ is a vector of shape $[1, m + 1]$ whereas $\\theta^T$ is a vector of shape $[m + 1, 1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LinearRegression(Model):\n",
    "    '''\n",
    "    DESCRIPTION:\n",
    "    \n",
    "    Parameters\n",
    "    features   - a testing set of size (nsamples, nfeatures)\n",
    "                 each sample within the set consists of a vector of size (1, nfeatures)\n",
    "    parameters - a parameter set of size (1, nfeatures + 1)\n",
    "    '''\n",
    "    def predict(self, features, parameters):\n",
    "        labels = [ ]\n",
    "        \n",
    "        for i in features:\n",
    "            i = np.asarray(i)       # convert to array if single-valued\n",
    "            i = np.insert(i, 0, 1)  # insert x0 = 1 into our feature vector\n",
    "            \n",
    "            p = np.atleast_2d(parameters)\n",
    "            \n",
    "            y = np.dot(np.transpose(p), i)\n",
    "            \n",
    "            labels.append(y)\n",
    "            \n",
    "        return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q. \"Argh! This seems to make no sense.\"\n",
    "**ST**\n",
    "\n",
    "For a Univariate Linear Regressor (like the example above) i.e. for $j = 0$ and for $j = 1$, the predict function would be:\n",
    "\n",
    "$$predict_{\\theta}(x_{1}^{(i)}) = \\theta_{0} + \\theta_{1}x_{1}^{(i)}$$\n",
    "\n",
    "knowing that $x_{0}^{(i)} = 1$\n",
    "\n",
    "The above equation is **similar to that of the equation of a line in a two-dimensional space** with $\\theta_{0}$ being the y-intercept (how high or low it is from $y = 0$) and $\\theta_{1}$ being the slope (steepness) of our line. The generalized equation extends this linear incisiveness to higher dimensions that cannot be visualized. The univariate equation help us to understand the fundamentals behind linear predictive models and how it attempts to *\"draw fine distinctions\"*.\n",
    "\n",
    "For the sake of a clear explanation, we'll consider a Univariate Linear Regressor that can be extended towards a higher dimensional vector space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem Statement 1\n",
    "> *Justice Delayed is Justice Piled: A predictive model for the Indian Judiciary System*\n",
    "\n",
    "For our explanation, we'll be considering a data set ***\"Crime against Women during 2014 in India\"*** published by the Government of India on their website [www.data.gov.in](www.data.gov.in) under the Digital India initiative. We'll be considering data sets from various platforms in order to learn and observe (and maybe, find reasons and solutions to problems that can be visualized through the data set). The data set is open to use and can be downloaded from [here](https://data.gov.in/catalog/crime-against-women).\n",
    "\n",
    "We'll be using Pandas' (a Python library for Data Analysis) DataFrame Object that helps us to load, view and perform operations on our data set.\n",
    "\n",
    "Imagine if the employees of the Indian Judiciary System would like to know the number of cases that may be pending for investigation at the end of the year by knowing the number of cases that were pending investigation the year before.\n",
    "\n",
    "In simpler terms, *By how much will the burden of the Judiciary System rise for cases of crimes against women by the end of the year?*\n",
    "In order to serve justice well and as early as possible to victims of the crimes, our goal would be to decrease the number of pending investigations for the year in comparision to what our learning model (our Linear Regressor) predicts.\n",
    "\n",
    "Let's plot our graph and try to infere what we observe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlEAAAGICAYAAABhpbWkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRDmO4ziO4+TA/wO1OEsNonYbawAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc42cb6ca20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df         = pd.read_csv('./data/crimes-against-women-india-2014.csv') # loads the data set (a comma seperated value file) into a\n",
    "                                                                       # Pandas' DataFrame Object\n",
    "df         = df[df['States/UTs'] == 'Total (All India)']               # slice rows that denote only the overall consensus\n",
    "\n",
    "column1    = 'Cases pending investigation from previous year'\n",
    "column2    = 'Cases pending investigation at the end of the year'\n",
    "\n",
    "x          = np.atleast_2d(df[column1]).T                              # values belonging to our column 1\n",
    "y          = np.atleast_2d(df[column2]).T                              # values belonging to our column 2\n",
    "nsamples   = len(x)\n",
    "\n",
    "plt.scatter(x, y, marker = 'x')\n",
    "\n",
    "plt.xlabel(column1)\n",
    "plt.ylabel(column2)\n",
    "\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By observing our graph above, we can very much infere that there exists a clear rising **linearity** between the two variables. To prove this, we can calculate the *Pearson product-moment correlation coefficient* which in simpler terms, checks the degree of linearilty between our two variables on a scale of $[-1, +1]$ where $-1$ denotes a perfect negative linear correlation, $0$ denotes no linear correlation and $+1$ denotes a perfect positive linear correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.99879104])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r, p = scipy.stats.pearsonr(x, y)\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very impressive. We've recieved a correlation coefficient of $\\approx 0.9988$ which states the high degree of linearity which exists in our data set. Our goal is to fit a best-fit line through the data observed which depends on our parameter set $\\theta$\n",
    "\n",
    "Let's consider a few cases for our parameter set $\\theta$\n",
    "\n",
    "Case 1: $\\theta_{0} = 15$ and $\\theta_{1} = 0.0$\n",
    "\n",
    "Case 2: $\\theta_{0} = 20$ and $\\theta_{1} = 0.5$\n",
    "\n",
    "Case 3: $\\theta_{0} = 23$ and $\\theta_{1} = 1.5$\n",
    "\n",
    "We can then plot the above parameters with respect to the prediction function as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlkAAAF5CAYAAACsvD/sAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwzAMwzACwJwswzAMwzCMAPh/fKnbKAtIeWcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc42cb20e80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x, y, marker = '1')\n",
    "\n",
    "parameters  = np.asarray([\n",
    "    [[15], [0.0]], # case 1 \n",
    "    [[20], [0.5]], # case 2\n",
    "    [[23], [1.5]]  # case 3\n",
    "], dtype = np.float32)\n",
    "\n",
    "regressor   = LinearRegression()\n",
    "legends     = [ ]\n",
    "predictions = [ ]\n",
    "\n",
    "for p in parameters:\n",
    "    prediction = regressor.predict(x, p)\n",
    "    \n",
    "    plt.plot(x, prediction)\n",
    "    \n",
    "    legends.append('$\\\\theta_{0} = %.2f$ and $\\\\theta_{1} = %.2f$' % (p[0], p[1]))\n",
    "    \n",
    "    predictions.append(prediction)\n",
    "    \n",
    "plt.legend(legends)\n",
    "\n",
    "plt.xlabel(column1)\n",
    "plt.ylabel('$predict_{\\\\theta}(x_{1}^{(i)}) = \\\\theta_{0} + \\\\theta_{1}x_{1}^{(i)}$')\n",
    "\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that an increase in $\\theta_{0}$ raises the line above $y = 0$. Similarly an increase in $\\theta_{1}$ increases the radial shift of our line with respect to $x = 0$. Our goal is to choose $\\theta_{0}$ and $\\theta_{1}$ such that it is close to output $y$ for any given input $x$,\n",
    "\n",
    "i.e. the difference between the output from the prediction $predict_{\\theta}(x_{j}^{(i)})$ and $y_{k}^{(i)}$ must be mimimum. Hence, for a given training set with $m$ training samples, we could then estimate the difference in the errors (a cost function) by,\n",
    "\n",
    "$$C(\\theta) = \\frac{1}{2m}\\sum_{i = 1}^{m} (predict_{\\theta}(x^{(i)}) - y^{(i)})^{2}$$\n",
    "\n",
    "Such a function is also called as the Mean Squared Error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean_squared_error(output, target, nsamples):\n",
    "    sum    = np.sum(np.power(np.subtract(output, target), 2))\n",
    "    result = (0.5 / nsamples) * sum\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ST**\n",
    "\n",
    "The quadratic nature of the cost function speaks volumes. Squaring the difference helps us eliminate the positive and negative feedback of errors and at the same time increase the *spread* on a wide range. The factor $\\frac{1}{2}$ is multiplied with the mean cost since minimizing the overall error is same as minimizing half its error. A clear reason as to why we've chosen the MSE as our cost function will be discussed in some time.\n",
    "\n",
    "**Our job is to find a parameter set $\\theta$ that could minimize the cost function close or equal to 0.**\n",
    "\n",
    "Let's consider our Univariate Linear Regressor again, this time considering $\\theta_{0}$ constant (i.e., having no affect over our cost function $C(\\theta)$) and estimate the cost with respect to the parameters we've used.\n",
    "\n",
    "Knowing $\\theta_{0} = 0.0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$Case 1: C(0.00, 0.00) = 3.39e+08$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$$Case 2: C(0.00, 0.50) = 1.11e+08$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$$Case 3: C(0.00, 1.50) = 2.84e+07$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters  = np.array(\n",
    "    [[[0.0], [0.0]], # case 1\n",
    "     [[0.0], [0.5]], # case 2\n",
    "     [[0.0], [1.5]]] # case 3\n",
    ", dtype = np.float32)\n",
    "predictions = [ ]\n",
    "\n",
    "for p in parameters:\n",
    "    prediction = regressor.predict(x, p)\n",
    "    predictions.append(prediction)\n",
    "\n",
    "for i in range(len(predictions)):\n",
    "    error = mean_squared_error(predictions[i], y, nsamples)\n",
    "    display(Math(\"Case %i: C(%.2f, %.2f) = %.2e\" % (i + 1, parameters[i][0], parameters[i][1], error)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were to consider for a range of $\\theta_{1}$ values (keeping our $\\theta_{0}$ as it is), we can then plot a graph like the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiIAAAGDCAYAAAAWHHv8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc42cab8da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$$C_{min}(0.00, 1.20) = 9.93e+05$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "theta1    = np.arange(-10, 10, 0.1) # a range of theta1\n",
    "mintheta1 = sys.maxsize\n",
    "minerror  = sys.maxsize\n",
    "\n",
    "for t in theta1:\n",
    "    prediction = regressor.predict(x, [[0], [t]])\n",
    "    error      = mean_squared_error(prediction, y, nsamples)\n",
    "    \n",
    "    if error < minerror:\n",
    "        mintheta1 = t\n",
    "        minerror  = error\n",
    "    \n",
    "    plt.plot(t, error, marker = '.', c = 'b')\n",
    "\n",
    "plt.xlabel('$\\\\theta_{1}$')\n",
    "plt.ylabel('$C(\\\\theta_{1})$')\n",
    "\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "display(Math(\"C_{min}(0.00, %.2f) = %.2e\" % (mintheta1, minerror)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woah! Check out that graph. We've got a smooth decrease from an error at around $3.2 \\times 10^{10}$ at $\\theta_{1} = -9.99$ reaching $9.93 \\times 10^{5}$ at $\\theta_{1} = 1.20$ and a gradual increase in error again.\n",
    "\n",
    "If we were to move one more dimension higher, we then could estimate the cost with respect to both, $\\theta_{0}$ and $\\theta_{1}$. Let's consider for a range of $\\theta_{0}$ values as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "theta0    = np.arange(-10, 10, 0.1)\n",
    "mintheta0 = sys.maxsize\n",
    "mintheta1 = sys.maxsize\n",
    "minerror  = sys.maxsize\n",
    "\n",
    "errors    = [ ]\n",
    "\n",
    "for u in theta0:\n",
    "    l = [ ]\n",
    "    for v in theta1:\n",
    "        prediction = regressor.predict(x, [[u], [v]])\n",
    "        error      = mean_squared_error(prediction, y, nsamples)\n",
    "\n",
    "        if error < minerror:\n",
    "            mintheta0 = u\n",
    "            mintheta1 = v\n",
    "            minerror  = error\n",
    "            \n",
    "        l.append(error)\n",
    "            \n",
    "    errors.append(l)\n",
    "\n",
    "display(Math(\"$C_{min}(%.2f, %.2f) = %.2e$\" % (mintheta0, mintheta1, minerror)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm. Well, the error seems to have reached its minimum at $9.92 \\times 10^{5}$ after being influenced by $\\theta_{0}$. Raising yet another dimension, we could visualize a 3-dimensional plot with contours reflected at each axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_surface(x, y, z, labels = { 'x': '', 'y': '', 'z': '' }):\n",
    "    axes = plt.gca(projection = '3d')\n",
    "    axes.plot_surface(x, y, z, alpha = 0.1)\n",
    "\n",
    "    axes.set_xlabel(labels['x'])\n",
    "    axes.set_ylabel(labels['y'])\n",
    "    axes.set_zlabel(labels['z'])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "\n",
    "X, Y   = np.meshgrid(theta0, theta1)\n",
    "Z      = np.atleast_2d(errors)\n",
    "\n",
    "labels = { 'x': '$\\\\theta_{0}$', 'y': '$\\\\theta_{1}$', 'z': '$C(\\\\theta_{0}, \\\\theta_{1})$' }\n",
    "plot_surface(X, Y, Z, labels)\n",
    "\n",
    "axes   = plt.gca(projection = '3d')\n",
    "# plotting Cmin\n",
    "axes.scatter(mintheta0, mintheta1, minerror)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "axes = plt.gca(projection = '3d')\n",
    "\n",
    "axes.contour(X, Y, Z, zdir = 'x', offset = np.amin(X), cmap = cm.coolwarm)\n",
    "axes.contour(X, Y, Z, zdir = 'y', offset = np.amax(Y), cmap = cm.coolwarm)\n",
    "axes.contour(X, Y, Z, zdir = 'z', offset = np.amin(Z), cmap = cm.coolwarm)\n",
    "\n",
    "axes.set_xlabel('$\\\\theta_{0}$')\n",
    "axes.set_ylabel('$\\\\theta_{1}$')\n",
    "axes.set_zlabel('$C(\\\\theta_{0}, \\\\theta_{1})$')\n",
    "\n",
    "plt.title('Contours')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q. \"Great! We can now plug in these parameters and have our best fit line.\"\n",
    "\n",
    "Not really. In our first example, we considered $\\theta_{0} = 0$ whereas in our second example we limited our range for $\\theta_{0} = [-10, 10)$. Both of these examples have resulted in a different minimized error, i.e. we limited our sack of $\\theta$ values and then went on a hunt for our parameters that minimizes our cost function. Many sacks of $\\theta$s results many minimized errors but then, what would be our **optimal solution**? Our approach of trying many permutations of these ranges is not a great one. So let us refer to our graph once again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_surface(X, Y, Z, labels)\n",
    "\n",
    "# plotting Cmin\n",
    "axes = plt.gca(projection = '3d')\n",
    "axes.scatter(mintheta0, mintheta1, minerror)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our minimized error (blue dot) is no way near to the steepest point downhill that surface. What could then be another solution to reach to our steepest point?\n",
    "\n",
    "**ST**\n",
    "\n",
    "One way to realize this problem is to imagine a scenario wherein you are placed at a random point on the graph. Consider taking small iterative steps such that you're moving all the way dowhill upto a point wherein whichever step you may take in any direction does not lead you further downhill.\n",
    "\n",
    "In mathematics, we call this as the **Gradient Descent Algorithm** which in simpler terms means *\"move downhill by taking very small steps until you cannot move further downhill.\"* We call this least downhill point (with respect to the observer) as a **point of convergence**.\n",
    "\n",
    "### Gradient Descent Algorithm\n",
    "\n",
    "We can now formulate our Gradient Descent Algorithm as follows:\n",
    "\n",
    "Let $\\alpha$ be the learning rate (how small our step must be), then\n",
    "\n",
    "$repeat$\n",
    "$$\\theta_{j} := \\theta_{j} - \\alpha\\frac{\\partial C(\\theta)}{\\partial \\theta_{j}}$$\n",
    "$until$ $convergence$\n",
    "\n",
    "One must note that this is a parallel operation, i.e. each of these assignments are updated simultaneously. (*we look towards all possible directions and only then take a step*)\n",
    "\n",
    "**ST**\n",
    "\n",
    "The $:=$ operator is an assignment operator (updating a new value for the said variable) whereas the $\\frac{\\partial C(\\theta)}{\\partial \\theta_{j}}$ factor denotes the rate of change of our cost function influenced by all variables in the parameter set $\\theta$ with respect to an individual parameter $\\theta_{j}$.\n",
    "\n",
    "Consider this factor to be somewhat a GPS compass (covering a limited surface area). Our GPS is completely aware of the shape of our hill and guides us which direction should we move towards. If the factor were to be a positive large value, then our step would be equally large moving towards a lesser (big jump) $\\theta_{j}$ value. Similarly, if our factor to be a negative small value, our step would be moving towards a greater (small jump) $\\theta_{j}$ value.\n",
    "\n",
    "One needs to observe for the fact that if $\\alpha$ (our learning parameter) were to be very small, the algorithm would take a great amount of time to reach a point of convergence. Whereas in case where the $\\alpha$ were to be comparatively a larger value, we may overshoot the point of convergence and may even tend to diverge.\n",
    "\n",
    "Let's consider $C(\\theta)$ and compute its partial derivative with respect to $\\theta_{j}$, we then get.\n",
    "\n",
    "$$\\frac{\\partial C(\\theta)}{\\partial \\theta_{j}} = \\frac{1}{m} \\sum_{i = 1}^{m} (predict_{\\theta}(x^{(i)}) - y^{(i)}) \\cdot x_{j}^{(i)}$$\n",
    "\n",
    "##### Q. \"Hmm. This seems to be a better approach. Can we now plug in the parameter values we recieve from GDA?\"\n",
    "\n",
    "Well, yes. Our graph has a single point of convergence which denotes the **global minima** simply because of the quadratic nature of our function $C(\\theta)$. On our two-dimensional graph, we observed a parabolic plot (a convex function) and raising a parabola to higher dimensions thereby also results a convex function. No matter what, we'll always have a single point of convergence. So yes. We must thank our quadratic cost function for helping us get away with local minimas.\n",
    "\n",
    "We can now implement our Gradient Descent Algorithm as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def gradient_descent(features, labels, parameters, nsamples, predict, learning_rate = 0.001):\n",
    "#     predictions = predict(features, parameters)\n",
    "#     error       = predictions - labels\n",
    "    \n",
    "#     features    = np.insert(features, 0, 1, axis = 1)\n",
    "    \n",
    "#     dcost       = (1.0 / nsamples) * np.dot(np.transpose(error), features)\n",
    "#     parameters  = parameters - learning_rate * np.transpose(dcost)\n",
    "    \n",
    "#     return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# iterations = 10\n",
    "# parameters = np.random.randn(2, 1)\n",
    "\n",
    "# for i in range(iterations):\n",
    "#     parameters = gradient_descent(x, y, parameters, nsamples, regressor.predict)\n",
    "#     prediction = regressor.predict(x, parameters)\n",
    "#     error      = mean_squared_error(prediction, y, nsamples)\n",
    "#     print(error)\n",
    "    \n",
    "#     plt.plot(i, error, marker = '.', color = 'blue')\n",
    "    \n",
    "# plt.grid()\n",
    "# plt.show()\n",
    "\n",
    "# plt.scatter(x, y)\n",
    "# plt.plot(x, regressor.predict(x, parameters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's discuss another kind of Supervised Learning problems.\n",
    "\n",
    "#### 3.A.b Classification\n",
    "\n",
    "Let's consider yet another data set to work on in order to understand what Classification is all about. We'll consider a famous data set provided by `scikit-learn` - [Ronald Fisher's **iris** data set](https://en.wikipedia.org/wiki/Iris_flower_data_set) provided under `load_iris()`. The data set maps 4 different attributes namely (sepal length & width, petal length & width) to a given iris species from a set of 3 (setosa, versicolor and virginica)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris    = load_iris()\n",
    "x       = iris.data[:,0] # sepal length (cm)\n",
    "y       = iris.data[:,1] # petal length (cm)\n",
    "\n",
    "plt.scatter(x, y, marker = 'x', c = iris.target)\n",
    "plt.xlabel('sepal length (cm)')\n",
    "plt.ylabel('petal length (cm)')\n",
    "    \n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of the above graph, we see that ***setosa*** species (blue) has a small sepal length with a comparatively larger petal length, whereas ***virginica*** species (red) has a larger sepal length but a smaller petal length in comparision. Such a kind of problem could be termed as a **Classification** problem, i.e. choosing a categorical output based on some input variables (categorical or numeric). If we were to draw boundaries within the scatter plot, we could approximately classify a given iris species from a list of 3 based on the sepal length and petal length provided.\n",
    "\n",
    "Machine Learning is all about building those boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.A.b.i Logistic Regression\n",
    "\n",
    "#### Logistical Function\n",
    "A logistical function can be denoted as:\n",
    "$$f(x) = \\frac{H}{1 + e^{-k(x - x_{0})}}$$\n",
    "where $H$ is the maximum value, $k$ is its steepness and $x_{0}$ is the value of the function's mid-point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic(x, max = 1, steepness = 1, mid = 0):\n",
    "    return max / (1 + np.exp(- steepness * (x - mid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# testing our logistic function using a range(-10, 10, 0.01)\n",
    "x = np.arange(-10, 10, 0.1)\n",
    "s = np.arange(0, 1, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plotting a sigmoid function and its first order derivative\n",
    "_, ax   = plt.subplots()\n",
    "legends = [ ] \n",
    "\n",
    "for i in s:\n",
    "    ax.plot(x, logistic(x, steepness = i), lw = 2)\n",
    "    legends.append('$k = %.2f$' % i)\n",
    "\n",
    "ax.grid(True, which = 'both')\n",
    "\n",
    "ax.axhline(y = 0, color = 'k')\n",
    "ax.axvline(x = 0, color = 'k')\n",
    "\n",
    "plt.legend(legends)\n",
    "\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel(\"$\\f(x)\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Artificial Neural Networks\n",
    "\n",
    "An Artificial Neural Network (hereby, ANN) is a learning model that is based on the way a Biological Neural Network works.\n",
    "\n",
    "Such models prove effective in cases wherein our data set is not *linearly seperable* i.e., one cannot seperate input to output mappings by merely drawing a line.\n",
    "\n",
    "#### Biological Neuron\n",
    "Our biological neural network system consists of billions of interconnected small units called as neurons. Let's look at a simplistic biological neuron and visualize our artifical model from it later.\n",
    "\n",
    "![](./img/biological-neuron.png)\n",
    "\n",
    "So, how do ANNs work? We now describe the smallest computational unit of an ANN - *neuron*.\n",
    "\n",
    "#### McCulloch-Pitts' Neuron\n",
    "\n",
    "![](./img/neuron.png)\n",
    "\n",
    "Image Source: [[6]](#References)\n",
    "\n",
    "Each neuron is connected to other neurons by a connection we call as the **synapse**. Each synapse has a weight $w$ associated to it. Consider this as the strength of the neuron that can be updated to increase the performance of the architecture. At each neuron, a $net$ is calculated by a summation of the products of each input to the neuron through a synapse and the weight associated to it on that synapse. The output of a neuron is given by $\\phi(net)$ where $\\phi$ denotes the activation function of that neuron. An additional neuron known as the **bias** having an input $+1$ is attached to each neuron of an ANN.\n",
    "\n",
    "Let $w_{ij}$ be the weight of a synapse/connection from the $i^{th}$ neuron to the $j^{th}$ and $b_{j}$ is the bias weight to the $j^{th}$ neuron. Let the input from the $i^{th}$ neuron be $x_i$. Then, $net_j$ can be defined as\n",
    "$$net_j = \\sum_{i = 1}^{m} w_{ij} x_i + b_{j}$$\n",
    "where $m$ is the number of neurons in the layer where neuron $i$ exists.\n",
    "\n",
    "The output from the $j_{th}$ neuron will then be\n",
    "$$x_{j} = \\phi(net_{i})$$\n",
    "\n",
    "where $\\phi(net_{j})$ is the activation function present at neuron $i$. Also, $x_{j}$ either acts as the output or an input from neuron $j$ to say, neuron $k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ST**\n",
    "\n",
    "Assume a neuron having a single input, say $x_{1}$ coming from neuron $1$ to neuron $2$ connected by a weight $w_{12}$. Also, declaring a bias input having an input 1 and a weight $b_{j}$ associated to its synapse. We can then formulate the output $y$ at neuron $2$ as,\n",
    "$$y = \\phi(w_{12} x_1 + b_{2})$$\n",
    "\n",
    "We notice that the above equation represents a Univariate Logistic Regressor, where $w_{12}$ denotes the slope of a line and $b_{2}$ denotes the y-intercept. Hence, each ***neuron in itself is a logistic unit.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.A Activation Functions\n",
    "\n",
    "We provide an exhaustive list of various activation functions and their implementations.\n",
    "- [Identity](#Identity-Function)\n",
    "- [Heaviside](#Heaviside-Function)\n",
    "- [Sigmoid](#Sigmoid-Function)\n",
    "- [Bipolar Sigmoid](#Bipolar-Sigmoid-Function)\n",
    "- [Complementary log-log](#Complementary-log-log-Function)\n",
    "- [Hyperbolic Tan](#Hyperbolic-Tan-Function)\n",
    "- [Yann LeCun's Hyperbolic Tan](#Yann-LeCun's-Hyperbolic-Tan-Function)\n",
    "- [ReLU (Rectifier Linear Unit) / Ramp](#ReLU-Function)\n",
    "- [Absolute](#Absolute-Function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identity Function\n",
    "$$\\phi(x) = x$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# an identity function\n",
    "def identity(x, derivative = False):\n",
    "    if derivative:\n",
    "        return np.sign(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# testing our activation functions using a range(-10, 10, 0.01)\n",
    "x = np.arange(-10, 10, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plotting an identity function and its first order derivative\n",
    "_, ax = plt.subplots()\n",
    "ax.plot(x, identity(x)                   , lw = 2)\n",
    "ax.plot(x, identity(x, derivative = True), lw = 2)\n",
    "\n",
    "ax.grid(True, which = 'both')\n",
    "\n",
    "ax.axhline(y = 0, color = 'k')\n",
    "ax.axvline(x = 0, color = 'k')\n",
    "\n",
    "plt.title('Identity Function')\n",
    "\n",
    "plt.legend([\"$\\phi(x)$\", \"$\\phi'(x)$\"])\n",
    "\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel(\"$\\phi(x)/\\phi'(x)$\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heaviside Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a heaviside function\n",
    "def heaviside(x):\n",
    "    return 0.5 * (np.sign(x) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plotting a heaviside function\n",
    "_, ax = plt.subplots()\n",
    "ax.plot(x, heaviside(x), lw = 2)\n",
    "\n",
    "ax.grid(True, which = 'both')\n",
    "\n",
    "ax.axhline(y = 0, color = 'k')\n",
    "ax.axvline(x = 0, color = 'k')\n",
    "\n",
    "plt.title('Heaviside Function')\n",
    "\n",
    "plt.legend([\"$\\phi(x)$\"])\n",
    "\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel(\"$\\phi(x)$\")\n",
    "\n",
    "# adding margins\n",
    "margin         = 1\n",
    "x1, x2, y1, y2 = plt.axis()\n",
    "plt.axis((x1, x2, y1 - margin, y2 + margin))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sigmoid Function\n",
    "$$\\phi(x) = \\frac{1}{1 + e^{-x}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# finding the first order derivative of a sigmoid function\n",
    "s = sp.Symbol('x')            # denoting a sympy symbol\n",
    "f = 1 / (1 + sp.exp(-s))      # a sigmoid function\n",
    "sp.diff(f)                    # differentiating the equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a sigmoid function\n",
    "def sigmoid(x, derivative = False):\n",
    "    e = np.exp(-x)\n",
    "    \n",
    "    if derivative:\n",
    "        return e / np.power(1 + e, 2)\n",
    "    return 1 / (1 + e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plotting a sigmoid function and its first order derivative\n",
    "_, ax = plt.subplots()\n",
    "ax.plot(x, sigmoid(x)                   , lw = 2)\n",
    "ax.plot(x, sigmoid(x, derivative = True), lw = 2)\n",
    "\n",
    "ax.grid(True, which = 'both')\n",
    "\n",
    "ax.axhline(y = 0, color = 'k')\n",
    "ax.axvline(x = 0, color = 'k')\n",
    "\n",
    "plt.title('Sigmoid Function')\n",
    "\n",
    "plt.legend([\"$\\phi(x)$\", \"$\\phi'(x)$\"])\n",
    "\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel(\"$\\phi(x)/\\phi'(x)$\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bipolar Sigmoid Function\n",
    "$$\\phi(x) = \\frac{1 - e^{-x}}{1 + e^{-x}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# finding the first order derivative of a bipolar sigmoid function\n",
    "f = (1 - sp.exp(-s)) /  (1 + sp.exp(-s))     # a bipolar sigmoid function\n",
    "d = sp.diff(f)                               # differentiating the equation\n",
    "sp.simplify(d)                               # simplifying the equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a bipolar sigmoid function\n",
    "def bipolar_sigmoid(x, derivative = False):\n",
    "    e = np.exp(-x)\n",
    "    \n",
    "    if derivative:\n",
    "        e = np.exp(x)\n",
    "        return 2 * e / np.power(1 + e, 2)\n",
    "    return (1 - e) / (1 + e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plotting a sigmoid function and its first order derivative\n",
    "_, ax = plt.subplots()\n",
    "ax.plot(x, bipolar_sigmoid(x)                   , lw = 2)\n",
    "ax.plot(x, bipolar_sigmoid(x, derivative = True), lw = 2)\n",
    "\n",
    "ax.grid(True, which = 'both')\n",
    "\n",
    "ax.axhline(y = 0, color = 'k')\n",
    "ax.axvline(x = 0, color = 'k')\n",
    "\n",
    "plt.title('Bipolar Sigmoid Function')\n",
    "\n",
    "plt.legend([\"$\\phi(x)$\", \"$\\phi'(x)$\"])\n",
    "\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel(\"$\\phi(x)/\\phi'(x)$\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Complementary log-log Function\n",
    "$$\\phi(x) = 1 - e^{-e^{x}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# finding the first order derivative of a cloglog function\n",
    "f = 1 - sp.exp(-sp.exp(s))        # a cloglog function\n",
    "d = sp.diff(f)                    # differentiating the equation\n",
    "sp.simplify(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a complementary log-log function\n",
    "def cloglog(x, derivative = False):\n",
    "    e = np.exp(x)\n",
    "    \n",
    "    if derivative:\n",
    "        return np.exp(x - e)\n",
    "    return 1 - np.exp(-e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plotting a sigmoid function and its first order derivative\n",
    "_, ax = plt.subplots()\n",
    "ax.plot(x, cloglog(x)                   , lw = 2)\n",
    "ax.plot(x, cloglog(x, derivative = True), lw = 2)\n",
    "\n",
    "ax.grid(True, which = 'both')\n",
    "\n",
    "ax.axhline(y = 0, color = 'k')\n",
    "ax.axvline(x = 0, color = 'k')\n",
    "\n",
    "plt.title('Complementary Log Log')\n",
    "\n",
    "plt.legend([\"$\\phi(x)$\", \"$\\phi'(x)$\"])\n",
    "\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel(\"$\\phi(x)/\\phi'(x)$\")\n",
    "\n",
    "# adding margins\n",
    "margin         = 0.5\n",
    "x1, x2, y1, y2 = plt.axis()\n",
    "plt.axis((x1, x2, y1 - margin, y2 + margin))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperbolic Tan Function\n",
    "$$\\phi(x) = tanh(x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# finding the first order derivative of a tanh function\n",
    "f = sp.tanh(s)\n",
    "sp.diff(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a tanh function\n",
    "def tanh(x, derivative = False):\n",
    "    if derivative:\n",
    "        return 1 - np.power(np.tanh(x), 2)\n",
    "    return np.tanh(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plotting a tanh function and its first order derivative\n",
    "_, ax = plt.subplots()\n",
    "ax.plot(x, tanh(x)                   , lw = 2)\n",
    "ax.plot(x, tanh(x, derivative = True), lw = 2)\n",
    "\n",
    "ax.grid(True, which = 'both')\n",
    "\n",
    "ax.axhline(y = 0, color = 'k')\n",
    "ax.axvline(x = 0, color = 'k')\n",
    "\n",
    "plt.title('Hyperbolic Tan Function')\n",
    "\n",
    "plt.legend([\"$\\phi(x)$\", \"$\\phi'(x)$\"])\n",
    "\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel(\"$\\phi(x)/\\phi'(x)$\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Yann LeCun's Hyperbolic Tan Function\n",
    "This activation function is best to use when:\n",
    "* Our data set to be fed into the network is *normalized*.\n",
    "* Weights of our network are initialized using Yann LeCun's weight initialization method.\n",
    "\n",
    "For more details, see [[7]](#References)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tanh_lecun(x, derivative = False):\n",
    "    a = 1.7159\n",
    "    b = (2.0 / 3.0)\n",
    "    \n",
    "    if derivative:\n",
    "        return a * (b - np.tanh(b * x))\n",
    "    return a * np.tanh(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plotting yann lecun's tanh function and its first order derivative\n",
    "_, ax = plt.subplots()\n",
    "ax.plot(x, tanh_lecun(x)                   , lw = 2)\n",
    "ax.plot(x, tanh_lecun(x, derivative = True), lw = 2)\n",
    "\n",
    "ax.grid(True, which = 'both')\n",
    "\n",
    "ax.axhline(y = 0, color = 'k')\n",
    "ax.axvline(x = 0, color = 'k')\n",
    "\n",
    "plt.title(\"Yann LeCun's Hyperbolic Tan Function\")\n",
    "\n",
    "plt.legend([\"$\\phi(x)$\", \"$\\phi'(x)$\"])\n",
    "\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel(\"$\\phi(x)/\\phi'(x)$\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ReLU Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# a relu (Rectifier Linear Unit) function\n",
    "def relu(x, derivative = False):\n",
    "    if derivative:\n",
    "        # the derivative of a ramp function is nothing but a heaviside function\n",
    "        return heaviside(x)\n",
    "    return np.maximum(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plotting a relu function and its first order derivative\n",
    "_, ax = plt.subplots()\n",
    "ax.plot(x, relu(x)                   , lw = 2)\n",
    "ax.plot(x, relu(x, derivative = True), lw = 2)\n",
    "\n",
    "ax.grid(True, which = 'both')\n",
    "\n",
    "ax.axhline(y = 0, color = 'k')\n",
    "ax.axvline(x = 0, color = 'k')\n",
    "\n",
    "plt.title(\"ReLU Function\")\n",
    "\n",
    "plt.legend([\"$\\phi(x)$\", \"$\\phi'(x)$\"])\n",
    "\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel(\"$\\phi(x)/\\phi'(x)$\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Absolute Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# an absolute function\n",
    "def absolute(x):\n",
    "    return np.abs(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plotting a relu function and its first order derivative\n",
    "_, ax = plt.subplots()\n",
    "ax.plot(x, absolute(x), lw = 2)\n",
    "\n",
    "ax.grid(True, which = 'both')\n",
    "\n",
    "ax.axhline(y = 0, color = 'k')\n",
    "ax.axvline(x = 0, color = 'k')\n",
    "\n",
    "plt.title(\"Absolute Function\")\n",
    "\n",
    "plt.legend([\"$\\phi(x)$\"])\n",
    "\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel(\"$\\phi(x)$\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Weight Initialization Methods\n",
    "1. Gaussian (Normal) Distribution with a standard deviation $\\sigma = 0.01$\n",
    "\n",
    "2. Yann LeCun's Weight Initialization Method: Yann LeCun states that if the data is been normalized and `tanh_lecun` is been used, then the weights could be initialized from a distribution having a mean $\\mu = 0$ and a standard deviation $\\sigma = {\\frac{1}{\\sqrt{m}}}$ where $m$ is the number of inputs to a node.\n",
    "\n",
    "3. Xavier / Glorot Weight Initialization Method: If $m$ is the number of fan-in nodes and $n$ is the number of fan-out nodes, then the weights could be initialized choosing a random value from a Uniform Distribution in the range $$U \\sim [\\,{\\sqrt{\\frac{6}{m + n}}}, {\\sqrt{\\frac{6}{m - n}}}]\\,$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Probability Density Function (Uniform Distribution)\n",
    "$$P(x) = \\frac{1}{b - a}$$\n",
    "\n",
    "##### Probability Density Function (Normal Distribution)\n",
    "$$P(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}}$$\n",
    "\n",
    "In order to randomize weights around 0, we pick out random values from the said probability density function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# a probability density function for a Normal Distribution\n",
    "def pdf_normal(x, mu = 0.0, sigma = 1.0):\n",
    "    c = 2 * np.power(sigma, 2)\n",
    "    return (1.0 / np.sqrt(np.pi * c)) * np.exp(-(np.power(x - mu, 2) / c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plotting a Uniform Distribution\n",
    "def plot_uniform(low = 0.0, high = 1.0, bins = 100, size = 1000, title = None):\n",
    "    range      = np.random.uniform(low, high, size)\n",
    "    _, bins, _ = plt.hist(range, bins, normed = True)\n",
    "    plt.legend(['$U \\sim [\\,%.2f, %.2f]\\,$' % (low, high)])\n",
    "    \n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# a test case to plot a Uniform Distribution\n",
    "plot_uniform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plotting a Normal Distribution and its PDF\n",
    "def plot_normal(mu = 0.0, sigma = 1.0, bins = 100, size = 1000, title = None):\n",
    "    range      = np.random.normal(mu, sigma, size)\n",
    "    _, bins, _ = plt.hist(range, bins, normed = True)\n",
    "    plt.plot(bins, pdf_normal(bins, mu, sigma), lw = 2, color = 'r')\n",
    "    \n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "        \n",
    "    plt.legend(['$\\mu = %0.2f, \\sigma = %0.2f$' % (mu, sigma)])\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# a test case to plot a Normal Distribution and its PDF\n",
    "plot_normal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Our Artificial Neural Network class\n",
    "class ANN:\n",
    "    # available weight initialization methods\n",
    "    RANDN          = 1\n",
    "    LECUN_UNIFORM  = 2\n",
    "    LECUN_NORMAL   = 3\n",
    "    XAVIER_UNIFORM = 4\n",
    "    XAVIER_NORMAL  = 5\n",
    "    \n",
    "    # available activation functions \n",
    "    IDENTITY       = identity\n",
    "    CLOGLOG        = cloglog\n",
    "    RELU           = relu\n",
    "    SIGMOID        = sigmoid\n",
    "    TANH           = tanh\n",
    "    TANH_LECUN     = tanh_lecun\n",
    "    HEAVISIDE      = heaviside\n",
    "    ABSOLUTE       = absolute\n",
    "    \n",
    "    '''\n",
    "    @arguments\n",
    "        sizes    - a list containing the number of neurons in each layer inclusive of the input and output layer.\n",
    "        wimethod - the initialization method for initializating weights and biases.\n",
    "    '''    \n",
    "    def __init__(self, sizes, wimethod = RANDN, debug = False):\n",
    "        self.log      = Log(debug) # a logger for debugging\n",
    "        \n",
    "        self.nlayers  = len(sizes) # number of layers inclusive of i/p layer\n",
    "        self.ninputs  = sizes[ 0]  # number of inputs\n",
    "        self.noutputs = sizes[-1]  # number of outputs\n",
    "        self.sizes    = sizes      # the topological structure of the neural network\n",
    "        self.neurons  = sum(sizes) # number of neurons within the architecture\n",
    "        \n",
    "        self.biases, self.weights = self._init_weights(wimethod)\n",
    "        \n",
    "        # logging initial weights\n",
    "        self.log.debug(\"ANN._init_weights: Initial Weights: \")        \n",
    "        if self.log.debuggable:\n",
    "            for i, w in enumerate(self.weights):\n",
    "                self.log.debug('weight(%i)' % (i + 1))\n",
    "                self.log.display(npa_to_latex_bmatrix_string(w))\n",
    "                \n",
    "        # logging initial biases                \n",
    "        self.log.debug(\"ANN._init_weights: Initial Biases: \")       \n",
    "        if self.log.debuggable:\n",
    "            for i, b in enumerate(self.biases):\n",
    "                self.log.debug('bias(%i)' % (i + 1))\n",
    "                self.log.display(npa_to_latex_bmatrix_string(b))\n",
    "                \n",
    "    def _init_weights(self, wimethod = RANDN):\n",
    "        sizes   = self.sizes\n",
    "        \n",
    "        # initialize weights to 0 and then \"break the symmetry\" [3]\n",
    "        weights = [np.zeros(shape = (x, y)) for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "        biases  = [np.zeros(shape = (1, y)) for y    in sizes[1:]]\n",
    "        \n",
    "        if wimethod is ANN.RANDN:\n",
    "            self.log.debug(\"ANN._init_weights: Initialization Weights using a Gaussian Distribution (mu = 0, sigma = 0.01).\")\n",
    "            # a random gaussian distribution with standard deviation = 0.01\n",
    "            mu      = 0.0\n",
    "            sigma   = 0.01\n",
    "            weights = [np.random.normal(size = (x, y), loc = mu, scale = sigma) for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "            biases  = [np.random.normal(size = (1, y))                          for y    in sizes[1:]]\n",
    "            \n",
    "            self.log.debug('ANN._init_weights: Plotting a Probability Density Function for weights.')\n",
    "            if self.log.debuggable:\n",
    "                plot_normal(mu = mu, sigma = sigma, title = 'Initial Weights Distribution for all Layers.');\n",
    "                \n",
    "        if wimethod is ANN.LECUN_UNIFORM:\n",
    "            self.log.debug(\"ANN._init_weights: Initialization Weights using Yann LeCun's method (Uniform Distribution).\")\n",
    "            # to be implemented\n",
    "            \n",
    "        if wimethod is ANN.LECUN_NORMAL:\n",
    "            self.log.debug(\"ANN._init_weights: Initialization Weights using Yann LeCun's method (Normal Distribution).\")\n",
    "            mu     = 0.0\n",
    "            \n",
    "            for i in range(self.nlayers - 1):\n",
    "                m     = sizes[i]               # fan-in\n",
    "                n     = sizes[i + 1]           # fan-out\n",
    "                \n",
    "                sigma = np.sqrt(1.0 / m)\n",
    "                \n",
    "                weights[i] = np.random.normal(size = (m, n)) * sigma\n",
    "                biases [i] = np.random.normal(size = (1, n))\n",
    "                \n",
    "                u     = i + 1 # layer x\n",
    "                v     = i + 2 # layer y\n",
    "                \n",
    "                self.log.debug('ANN._init_weights: Plotting a Probability Density Function for weights (%i, %i).' % (u, v))\n",
    "                if self.log.debuggable:\n",
    "                    plot_normal(mu = mu, sigma = sigma, title = 'Initial Weight Distribution for (%i, %i)' % (u, v))\n",
    "                                                                              \n",
    "        if wimethod is ANN.XAVIER_UNIFORM:\n",
    "            self.log.debug(\"ANN._init_weights: Initialization Weights using Xavier's method (Uniform Distribution).\")\n",
    "            \n",
    "            for i in range(self.nlayers - 1):\n",
    "                m = sizes[i]               # fan-in\n",
    "                n = sizes[i + 1]           # fan-out\n",
    "                \n",
    "                r = np.sqrt(6.0 / (m + n)) # range\n",
    "                \n",
    "                weights[i] = np.random.uniform(low = -r, high = r, size = (m, n))\n",
    "                biases [i] = np.random.uniform(low = -r, high = r, size = (1, n))\n",
    "                \n",
    "                u     = i + 1 # layer x\n",
    "                v     = i + 2 # layer y\n",
    "                \n",
    "                self.log.debug('ANN._init_weights: Plotting a Uniform Distribution for weights (%i, %i).' % (u, v))\n",
    "                if self.log.debuggable:\n",
    "                    plot_uniform(low = -r, high = r, title = 'Initial Weight Distribution for (%i, %i)' % (u, v))\n",
    "                \n",
    "        if wimethod is ANN.XAVIER_NORMAL:\n",
    "            self.log.debug(\"ANN._init_weights: Initialization Weights using Xavier's method (Normal Distribution).\")\n",
    "            mu = 0.0\n",
    "            \n",
    "            for i in range(self.nlayers - 1):\n",
    "                m     = sizes[i]               # fan-in\n",
    "                n     = sizes[i + 1]           # fan-out\n",
    "                \n",
    "                sigma = np.sqrt(3.0 / (m + n)) # standard deviation\n",
    "                \n",
    "                weights[i] = np.random.normal(size = (m, n)) * sigma\n",
    "                biases [i] = np.random.normal(size = (1, n))\n",
    "                \n",
    "                u     = i + 1 # layer x\n",
    "                v     = i + 2 # layer y\n",
    "                \n",
    "                self.log.debug('ANN._init_weights: Plotting a Probability Density Function for weights (%i, %i).' % (u, v))\n",
    "                if self.log.debuggable:\n",
    "                    plot_normal(mu = mu, sigma = sigma, title = 'Initial Weight Distribution for (%i, %i)' % (u, v))\n",
    "        \n",
    "        return (biases, weights)\n",
    "    \n",
    "    def view(self):\n",
    "        graph = nx.DiGraph()\n",
    "        \n",
    "        # to be implemented\n",
    "        \n",
    "        nx.draw(graph, node_color = 'y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class FNN(ANN):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        ANN.__init__(self, *args, **kwargs)\n",
    "        \n",
    "    def _feedforward(self, x, activation = ANN.SIGMOID):\n",
    "        self.log.debug('FNN._feedforward: Feeding an input: ' + str(x))\n",
    "        \n",
    "        table  = PrettyTable(['LAYER.', 'WEIGHT', 'BIAS', 'NET', 'f(NET)'])\n",
    "\n",
    "        x      = np.atleast_2d(x)\n",
    "        print(x)\n",
    "        \n",
    "        input  = [ ]                         # stores the inputs  to   each layer\n",
    "        output = [x]                         # stores the outputs from each layer\n",
    "\n",
    "        for i in range(self.nlayers - 1):\n",
    "            w = self.weights[i]\n",
    "            b = self.biases [i]\n",
    "            \n",
    "            z = np.add(np.dot(x, w), b)      # z   = x.w + b, the net of a given neuron\n",
    "            x = activation(z)                              # a   = f(z)   , triggering the activation function\n",
    "\n",
    "            input .append(z)\n",
    "            output.append(x)\n",
    "\n",
    "            table.add_row([i + 1,\n",
    "                           npa_to_latex_bmatrix_string(w),\n",
    "                           npa_to_latex_bmatrix_string(b),\n",
    "                           npa_to_latex_bmatrix_string(z),\n",
    "                           npa_to_latex_bmatrix_string(x)])\n",
    "\n",
    "        self.log.debug('FNN._feedforward: For each iteration.')\n",
    "        self.log.display(table.get_html_string())\n",
    "\n",
    "        self.log.debug('FNN._feedforward: Done. Model generated: ' + str(x))\n",
    "\n",
    "        return { 'input': input, 'output': output }\n",
    "        \n",
    "    def fit(self, features, labels, activation = ANN.SIGMOID, learning_rate = 0.01, epochs = 10000):\n",
    "        self.activation   = activation\n",
    "        \n",
    "        choice            = random.randrange(self.ninputs) # choosing a random feature to feed into the network\n",
    "        \n",
    "        # feeding the feature once through the network\n",
    "        # model states the inputs and outputs of each layer\n",
    "        model             = self._feedforward(features[choice], activation)\n",
    "        y                 = model['output'][-1] # the output recieved from the output layer\n",
    "                \n",
    "        # using mean squared error\n",
    "        error             = mse(labels[choice], y)\n",
    "        self.log.debug('FNN.fit: Error recieved: ' + str(error))\n",
    "        \n",
    "        # initiating the backpropagation algorithm\n",
    "        \n",
    "        # derivative of the cost function\n",
    "        # = - (expected_output - predicted_output) x f'(input to output layer)\n",
    "        delta             = (y - labels[choice]) * self.activation(model['input'][-1], derivative = True)        \n",
    "        self.log.debug('FNN.fit: BackPropagation Error from output layer: ' + str(delta))\n",
    "        \n",
    "        delta_weights     = [np.zeros(w.shape) for w in self.weights] # delta weights list\n",
    "        delta_biases      = [np.zeros(b.shape) for b in self.biases]  # delta biases  list\n",
    "        \n",
    "        table             = PrettyTable(['LAYER.', 'DELTA', 'ΔW', 'ΔB'])\n",
    "        round             = 4\n",
    "        \n",
    "        delta_weights[-1] = np.dot(np.transpose(model['output'][-2]), delta)\n",
    "        delta_biases [-1] = delta\n",
    "        \n",
    "        table.add_row([self.nlayers,\n",
    "                       npa_to_latex_bmatrix_string(np.round(delta            , round)),\n",
    "                       npa_to_latex_bmatrix_string(np.round(delta_weights[-1], round)),\n",
    "                       npa_to_latex_bmatrix_string(np.round(delta_biases [-1], round))])\n",
    "        \n",
    "        for i in range(2, self.nlayers):\n",
    "            z                 = model['input' ][-i]\n",
    "            weight            = self.weights[-i + 1]\n",
    "            \n",
    "            delta             = np.dot(delta, np.transpose(weight)) * self.activation(z, derivative = True)\n",
    "            \n",
    "            a                 = model['output'][-i - 1]\n",
    "            \n",
    "            delta_weights[-i] = np.dot(np.transpose(a), delta)\n",
    "            delta_biases [-i] = delta\n",
    "            \n",
    "            table.add_row([self.nlayers - i + 1,\n",
    "                           npa_to_latex_bmatrix_string(delta),\n",
    "                           npa_to_latex_bmatrix_string(delta_weights[-i]),\n",
    "                           npa_to_latex_bmatrix_string(delta_biases [-i])])\n",
    "        \n",
    "        self.log.debug('FNN.fit: For each BackPropagation iteration.')\n",
    "        self.log.display(table.get_html_string())\n",
    "        \n",
    "    def predict(self, features):\n",
    "        self.log.debug('FNN.predict: Attempting to predict the outputs.')\n",
    "        \n",
    "        outputs = [ ] # a list of outputs recieved for each feature\n",
    "        \n",
    "        for i in features:\n",
    "            # feeding the feature through the network\n",
    "            model  = self._feedforward(i, self.activation)\n",
    "            output = model['output'][-1]\n",
    "            \n",
    "            self.log.debug('FNN.predict: Prediction for ' + str(i) + ' recieved is ' + str(output))\n",
    "            \n",
    "            outputs.append(output)\n",
    "            \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features     = np.asarray([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "labels       = [[0], [1], [1], [0]]\n",
    "\n",
    "nfeatures    =  len(features[0])  # number of features\n",
    "nlabels      =  len(labels[0])    # number of labels\n",
    "sizes_hidden =  [3]               # 1 hidden layer with 3 neurons\n",
    "\n",
    "sizes        = [nfeatures] + sizes_hidden + [nlabels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fnn = FNN(sizes, wimethod = ANN.XAVIER_NORMAL, debug = True)\n",
    "fnn.fit(features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Neural Networks (RNNs)\n",
    "\n",
    "Consider the case of Feedforward Neural Networks. Each sample from the training set passed through the network thereby resulting a desired output. In no case did the next input to be fed showed any dependency with its previous sample. But what about training samples that exhibit relationships between their inputs? At the same time, what about a training sample that denotes an output dependent on not only the current sample but also its previous training samples? One of the answers to this are languages; and here is where Recurrent Nerual Networks comes to our rescue.\n",
    "\n",
    "A Recurrent Neural Network (hereby, RNN) is a kind of an Artificial Neural Network that considers the question *at which time-step did you feed your input?* This encourages us to use RNNs for **modelling sequences**.\n",
    "\n",
    "#### Architecture\n",
    "A *vanilla* Recurrent Neural Network's (hereby, RNN) architecture can be defined as follows:\n",
    "\n",
    "\\begin{align*}\n",
    "h_{t} &= \\theta\\hspace{2mm}\\phi(h_{t-1}) + \\theta_{x}x_{t} \\\\\n",
    "y_{t} &= \\theta_{y}\\phi(h_{t})\n",
    "\\end{align*}\n",
    "\n",
    "where\n",
    "$x_{t}$ is the input vector at time-step $t$, $h_t$ and $h_{t-1}$ denote the state of the hidden layer at time-step $t$ and $t-1$ respectively, $y_{t}$ denotes the output vector at time $t$, $\\theta_{x}$ denotes the parameter (weight) used to condition the input vector $x$ at time-step $t$, $\\theta_{h_{t-1}}$ denotes the parameter (weight) used to condition the hidden layer $h$ at time-step $t-1$, $\\theta_{y}$ denotes the parameter (weight) used to condition the hidden layer $h$ at time-step $t$ and $\\phi(x)$ denotes the activation function (typically, a non-linear function). Also, $h_{0}$ would be the initialization vector at time-step $t = 1$.\n",
    "\n",
    "A nice way to look at the architecture is the *\"time-step dependency\"* relationship between $h_{t}$ and $h_{t-1}$ it exhibits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The \"Vanishing\" and \"Exploding\" Gradient Problem\n",
    "\n",
    "In order to estimate the error recieved from the RNN, we'd have to consider the sum of error derivatives with respect to $\\theta$ generated at each time-step. Hence,\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial E}{\\partial \\theta} &= \\sum_{t = 1}^{S} \\frac{\\partial E_{t}}{\\partial \\theta} \\\\\n",
    "\\frac{\\partial E_{t}}{\\partial \\theta} &= \\sum_{k = 1}^{t} \\frac{\\partial E_{t}}{\\partial y_{t}} \\frac{\\partial y_{t}}{\\partial h_{t}} \\frac{\\partial h_{t}}{\\partial h_{k}} \\frac{\\partial h_{k}}{\\partial \\theta_{t}}\n",
    "\\end{align*}\n",
    "\n",
    "where $S$ is the number of time-steps. Notice how the last two partial derivative factors at each time-step also relies on the previous $k$ time-steps. The term $\\frac{\\partial h_{t}}{\\partial h_{k}}$ denotes the partial derivative of the hidden layer at time-step $t$ with respect to the partial derivative of all the time-steps previous to it and can be given as:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial h_{t}}{\\partial h_{k}} &= \\prod_{i = k + 1}^{t} \\frac{\\partial h_{i}}{\\partial h_{i - 1}} \\\\\n",
    "                                      &= \\prod_{i = k + 1}^{t} \\frac{\\partial (\\theta\\phi(h_{i-1}) + \\theta_{x}x_{i})}{\\partial h_{i - 1}} \\\\\n",
    "                                      &= \\prod_{i = k + 1}^{t} \\theta^{T} diag[\\phi'(h_{i - 1})]\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long Short-Term Memory (LSTMs)\n",
    "##### Solving the \"Vanishing\" and \"Exploding\" Gradient Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Genetic Algorithms\n",
    "> *Survival of the fittest* - Charles Darwin, *On the Origin of Species*\n",
    "\n",
    "Our ability to learn is in itself - evolutionary; and the principles behind Genetic Algorithms are no different. Of course, the very nature of evolution is nothing but cell optimization over time.\n",
    "\n",
    "#### 1. Selection\n",
    "We select individuals that seem a best fit to our objective function (or fitness function) that can survive for the next generation.\n",
    "#### 2. Crossover\n",
    "Our individuals of the next generation perform a genetic crossover (sharing genetic information) that aim to perform a better generation of individuals that can survive our fitness function.\n",
    "#### 3. Mutation\n",
    "Mutation occurs when there is a kind of deformation of genes by say, exposure to radiation. In practice, this can be achieved by randomizing our string to an extent to form new individuals that aim to try preserving some genetic diversity.\n",
    "#### 4. Sampling\n",
    "Our resultant genetic diversity now creates new offsprings from these individuals recieved.\n",
    "\n",
    "We can now formulate a generalized algorithm for Genetic Algorithms as follows:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Study 1\n",
    "> Performance Analysis of\n",
    "> 1. k Nearest Neighbours (k-NN)\n",
    "> 2. Artificial Neural Networks (ANN)\n",
    "> 3. Support Vector Machines (SVM)\n",
    "\n",
    "> Using Cotton Germplasm Classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index\n",
    "1. Problem Definition\n",
    "-  Domain Knowledge\n",
    "-  Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Problem Definition\n",
    "\n",
    "#### Statement\n",
    "> Given a data set containing 36 germplasm features corresponding to any one of the six groups (Marker, Variety, Okra, New, BBR and JBWR) of cotton, build an efficient prediction model that is able to ***classify*** under which group does a given input set of features lie within.\n",
    "\n",
    "#### Data Set\n",
    "Our data set is been provided by the Central Institute of Cotton Research (CICR), Nagpur, India. An initial insight into our acquired data set showed a high degree of unstructuredness, noise and ambiguities. Hence, a great amount of data preprocessing will be required only after which we will be able to analyse and profile our data set well.\n",
    "\n",
    "#### Motivation\n",
    "Based on our Problem Statement, our problem can be termed as a ***multi-class classification problem***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Domain Knowledge\n",
    "In some cases (such as ours), a decent domain knowledge would be necessary to help us understand and approach our data set better. I solemly declare myself to be naive in the field of seed science. After some research (basically Googling), I came across this Doctoral Thesis [[13]](#References) by Dr. Harish S. that can help us understand more about Cotton Germplasm. For instance, many discrepancies within the recieved data set were resolved thanks to a little understanding of what exactly our features denote. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data Preprocessing\n",
    "***Q. Why Preprocessing?***\n",
    "\n",
    "Data Preprocessing is possibly one of the most tedious and time-consuming part of building a prediction model. You can consider yourself lucky if you are working on a preprocessed data set. In real world however, a large amount of data sets are noisy in nature.\n",
    "\n",
    "As we have seen, Machine Learning algorithms although sound smart, but aren't smart enough. A raw data set holds a high dregree of noise (inconsistencies, discrepancies, etc.). The *GIGO* principle applies very well in our field of Machine Learning which states - ***\"garbage in, garbage out\"***. This means that if you were to feed inconsistent logic into your machine, you recieve equally flawed conclusions from it.\n",
    "\n",
    "#### Preprocessing through Human Intervention\n",
    "Making our data representable into a tabular format requires some kind of human intervention. I initially recieved the data set in an Excel sheet. After a day of cleaning we successfully converted our [dirty data](https://docs.google.com/spreadsheets/d/1LMYli4mkWrAuDSihFQ6w6raAJbEfebs3VlJUXT1wk88/pubhtml) to a [cleaned one](https://docs.google.com/spreadsheets/d/1PQlCQvYvdGDidxtgnAQeIbcY6fdKuBVeS-6LSxo2xtk/pubhtml)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading our data set\n",
    "Let's load our data set into a Pandas' DataFrame Object that can be further used to generate our training, validation and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = './data/cotton/cotton.csv'\n",
    "df   = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view a sample of our data set using `pandas.DataFrame.sample` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "number_of_samples = 3\n",
    "df.sample(number_of_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pandas_profiling` is a great library to generate a comphrehensive report of your data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas_profiling as pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "profile = pp.ProfileReport(df)\n",
    "profile.to_file(outputfile = './data/cotton/profile.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can view the complete profile over [here](./data/cotton/profile.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall follow a strict protocol [[12]](#References) of preprocessing our data which is as follows:\n",
    "1. Data Cleaning\n",
    "2. Data Integration\n",
    "3. Data Reduction\n",
    "4. Data Transformation\n",
    "\n",
    "`sklearn` has an entire module called `preprocessing` that can help us with preprocessing our data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Data Cleaning\n",
    "In order to increase the quality of our data set for analysis, some steps need to be considered for the following cases:\n",
    "1. Missing Values\n",
    "2. Duplicate Data\n",
    "3. Inconsistent Data\n",
    "4. Noise\n",
    "5. Outliers\n",
    "\n",
    "#### 1. Missing Values\n",
    "We can get away with missing values by either:\n",
    "1. Ignoring our missing values.\n",
    "2. Deleting training tuples that contain missing values.\n",
    "3. Substituting a value in place of a missing value.\n",
    "\n",
    "Ignoring our missing values is certainly not the option since not all predictive models consider missing values (k-NN in this case). Secondly, deleting our training tuples with missing values could remove a large amount of relavant data that could help our model to learn better. `pandas` has a neat function called `DataFrame.dropna()` if you wish to consider deleting missing values *(not recommended)*.\n",
    "\n",
    "Our third case would be clearly substituting a value in place of missing values. We call this as ***Imputation of Missing Values***. A wide range of stratergies and algorithms exists to impute missing values.\n",
    "\n",
    "1. Filling NA/NaN cells with the ***mean, median*** or ***mode*** of the corresponding feature vector: Considering the mean, median or mode (measures of center) is although plausible, but not necessarily efficient.\n",
    "2. Filling NA/NaN cells using clustering algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find the percentage of missing values present throughout the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(df.isnull().sum().sum() / df.size) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Around $7.88\\%$ of our entire data set contains missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df.fillna(df.mean())    # for numerical\n",
    "df.sample(number_of_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case of our numerical feature variables, we successfully imputed our missing values with the mean of that feature vector. Let's view the reduction of NA/NaN's present within our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(df.isnull().sum().sum() / df.size) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aha! We've reduced our number of missing values so far from about $7.88 \\%$ to $3.74 \\%$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df.fillna('NA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "`pandas` has a great plotting module extension to `matplotlib` that can help us visualizing mutli-variate data. Keeping our objective in mind, we can somewhat visualize our multi-variate data using n-dimensional visualizations, one being the Andrews Plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas.tools.plotting import andrews_curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Andrew's Plot is a plot of our features versus a smooth curve function, we would require to convert our data set into something numeric. In case of categorical data, one way to achieve our objective is to use `scikit-learn`'s `LabelEncoder` class that converts our categorical data corresponding to a numerical value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections           import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "le         = defaultdict(LabelEncoder)\n",
    "\n",
    "data       = df.copy()\n",
    "data       = data.apply(lambda x: x if x.dtype != 'object' else le[x.name].fit_transform(x))\n",
    "\n",
    "data.sample(number_of_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = data['Group'].unique()\n",
    "groups =   le['Group'].inverse_transform(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plot a mutli-dimensional visualization of our data as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_andrews_curve(df, target, labels):\n",
    "    plot       = andrews_curves(df, 'Group')\n",
    "    handles, _ = plot.get_legend_handles_labels()\n",
    "\n",
    "    plt.title(\"Andrew's Plot\")\n",
    "\n",
    "    plt.legend(handles, labels)\n",
    "\n",
    "    plt.xlabel('$t$')\n",
    "    plt.ylabel('$f_{x}(t)$')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_andrews_curve(data, 'Group', groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $$f_{x}(t) = \\frac{x^{(i)}_{0}}{\\sqrt{2}} + x^{(i)}_{1}\\sin{(t)} + x^{(i)}_{2}\\cos{(t)} + x^{(i)}_{3}\\sin{(2t)} + x^{(i)}_{3}\\cos{(2t)} + \\dots$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Argh! Plotting all our observations seem to overlap curves of some groups. Let's sample around 20% of our data set (random observations) for a better picture.\n",
    "\n",
    "Our data set consists of imbalanced observations for each class. We can calculate the percentage contribution of each group to our data set as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "frame = ((df['Group'].value_counts() / len(df)) * 100)\n",
    "frame.plot.pie(autopct = \"%.2f\", figsize = (6, 6))\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When it comes to sampling our data set (including splitting our data set into training and testing sets), the ratio of each group's contribution must be consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y         = data['Group'].copy()\n",
    "sample, _ = train_test_split(data, test_size = 0.9, stratify = y)\n",
    "\n",
    "plot_andrews_curve(sample, 'Group', groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference\n",
    "Andrew's Curves speaks volumes about our data set.\n",
    "* There aren't any irregular patterns within the curves of various groups which denotes absense of outliers.\n",
    "* Andrew's Curves highly depend on the order of importance of variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Data Transformation\n",
    "Our data set consists of a mixture of both categorical as well as numerical features. However, many machine learning models understand inputs only in terms of numbers. In order to convert our categorical data into numerical data, we need to use some kind of encoding system.\n",
    "\n",
    "Let's assume the three kinds of categorical data we can come across:\n",
    "* **Ordinal**, where the order of categories are important - for instance, the importance of $bad$ could be comparatively less than that of the importance of $good$.\n",
    "* **Nominal**, where the order of categories are not important - for instance, $apple$, $banana$ or $orange$, etc.\n",
    "* **Dichotomous**, where there exists just two categories - for instance, $male$ or $female$, $yes$ or $no$, etc.\n",
    "\n",
    "In case of ordinal variables, `scikit-learn` has the `LabelEncoder` class that can be used for this purpose. Unless you aren't using any kind of comparision of variables on the way, a simple `LabelEncoder` would suffice (for visualizations, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On analysing our data set, we can conclude that each categorical feature within our data set is clearly ***nominal*** and ***dichotomous*** in nature.\n",
    "\n",
    "Converting our nominal/dichotomous categories into something numeric, we use the **One-Hot Encoding** system. This kind of encoding system considers each nominal/dichotomous category as a seperate feature that takes a value either $1$ or $0$ if the corresponding feature holds that category or not.\n",
    "\n",
    "`pandas` helps us to convert our categorical data into One-Hot vectors using the `get_dummies` function. In statistics, a dummy variable is nothing but a variable that takes a value $1$ or $0$ to indiciate the presence or absense of some literal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, prefix_sep = ' ')\n",
    "df.sample(number_of_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such encoding schemes often tend to increase the number of features by a huge amount, thus adding a computation overhead on our learning models. In our case, these columns increased almost three times the initial number of columns (37 to 99)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "Assuming our data set, one possibly cannot compare say *fibre color* to *plant height*, that would be comparing apples to oranges. Our data set consists of incomparable features and hence must be squashed to some uniform range. We call this stage as ***Feature Scaling***.\n",
    "\n",
    "#### Normalization\n",
    "We can define our normalized data set within the range $[0, 1]$ as:\n",
    "$$X := \\frac{X - X_{min}}{X_{max} - X_{min}}$$\n",
    "\n",
    "where $X$ is our feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# normalization\n",
    "df = (df - df.min()) / (df.max() - df.min())\n",
    "df.sample(number_of_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "### Training/Testing Splits\n",
    "\n",
    "Our learning models require a unit that can quantify the overall performance of our model on our data set. Our training phase relies heavily on our data set and on our defined parameters (for parameteric models only) in order to build itself as a successful predictive model. However, using the entire data set would mean that our goal seems to be fitting our model to our data set alone, which was never the objective in the first place. Exhaustively utilizing the data set and tweaking parameters would possibly result in an ideal model and not a predictiion model. We call this as ***overfitting***, an ML engineer's worst nightmare.\n",
    "\n",
    "One good way to avoid this would be to split our data set into two parts of some defined ratio - a training set and a testing set. Random tuples from the data set can be added into our testing set that can be used to estimate how well our model performs for our data set. `scikit-learn` has a function called `train_test_split` under the `model_selection` module (in `scikit-learn 0.17` and before, the funciton is under the `cross_validation` module) that can divide a data set into a desired ratio of training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data set consists of *multi-class* target variables that are imbalanced by proportion within the data set. `scikit-learn`'s `train_test_split` function has a parameter `stratify` that divides our data set with the same proportion of our target variables as it was in the complete data set. (this is generally used for multi-class variables only)\n",
    "\n",
    "We use the Pareto Principle (80-20 rule of thumb), i.e, we divide our data set in the ratio of $80:20$ training/testing split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "columns_y = ['Group ' + group for group in groups]\n",
    "y         = df[columns_y].copy()\n",
    "\n",
    "train, test = train_test_split(data, stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nevertheless, the clutches of *overfitting* still cling on to us. During our training phase, our parameters are tweaked in order to fit on our training set. In someway, a leak of information of the testing set occurs during our training set, thereby leading to an overfit. In order to avoid this, we could go one step further by creating yet another set called as a *validation set* that can be used to estimate our model's performance after our training phase, then moving towards our testing phase.\n",
    "\n",
    "Now, a split of our data set into three seperate entities (training, testing and a validation set) might decrease the the number of required training tuples, thereby limiting the overall performance of our learning model.\n",
    "\n",
    "##### Q. \"Alright! So how do we really avoid overfitting?\n",
    "This brings us to the concept of **Cross Validation** (CV)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation\n",
    "Cross Validation helps us to answer the question, *\"how well does my learning model work on any independent data set?\"*\n",
    "\n",
    "#### K-fold Cross Validation\n",
    "We divide our data set into $K$ blocks. Then for each $i^{th}$ block (where $i = 1,2,...,K$), we assume the remaning $K - 1$ blocks to be our training set and $i$ to be our validation set for that given iteration. Hence, each block shall have $\\frac{u}{K}$ observations where $u$ is the total number of observations in our training set.\n",
    "\n",
    "A cumulative error (called as the Cross Validation Error) is calculated based on the errors recieved at each iteration from our validation set. Hence for any given model, our goal would be to minimize our Cross Validation Error.\n",
    "\n",
    "An ideal Cross Validation approach would be to divide our data set into $u$ blocks, i.e, $K = u$. We call this, **Leave One Out Cross Validation (LOOCV)**. For a data set with very limited observations, such an approach would be favourable but however in cases where we have millions of observations, an LOOCV would be computationally expensive. Typically, $K = 10$ (also known as 10-fold Cross Validation).\n",
    "\n",
    "`scikit-learn` has an extensive number of classes designated for various Cross Validation methods in its `model_selection` module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that our problem is a *multi-class classification* problem, our data set seems to hold an imbalanced number of samples for each given class. In order to ensure that for each fold, the relative frequencies of each class are preserved for our training and validation sets, `scikit-learn` has a decent class called `StratifiedKFold` which helps us to preserve the percentage of each sample for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "Not all features play an impressive role for our prediction model. We can get done away with features that are redundant and have no appreciable affect over our model's prediction. Feature Selection decreases the possibility of overfitting, improves our training time (lesser the number of features, lesser the computation required for learning), simplicity and relavancy.\n",
    "\n",
    "Our data set consists of 36 features, 1 label and 191 observations. While this may not sound a lot, a large number of data sets used for building prediction models may have an extensive number of features and observations. In our case, there may exists features that may impact the overall performance of our models.\n",
    "\n",
    "So, how do we go about it?\n",
    "\n",
    "Our first approach would be to find the best subset combination of various features which delivers the least error.\n",
    "\n",
    "#### All Subsets\n",
    "Let's initially assume our models require 0 features. Our observation would be in this case would be noise. As we consider various combination sets of features with respect to the error associated, we can then plot a graph as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-Nearest Neighbors (k-NN)\n",
    "\n",
    "Being one of the most simplest of all machine learning algorithms, k-Nearest Neighbors (in short, k-NN) is a no-brainer. k-NN is a non-parametric approach which in simpler terms means that its performance solely depends on our data set. This encourages a more flexible model that can distinguish boundaries even better.\n",
    "\n",
    "Why so?\n",
    "\n",
    "Remember in the case of our Linear Regressor, a single tweak in any one of the parameters in our parameter set led our entire boundary (lines or hyperplanes) to change with respect to our $x$'s and $y$'s. Imagine a function $f(x)$ which shows a linear relationship, followed by constant relationship and eventually rises exponentially. Clearly fitting a parametric model for a such a function would be herculean; which is why we have non-parametric models that aim to fit themselves within each of these local relationships that exists within a data set thereby having a high dependency on it. Also, this raises yet another criteria for non-parametric models that they possess a high dependency on a data set of sufficient size.\n",
    "\n",
    "k-Nearest Neighbors can be defined in a layman's terms as follows:\n",
    "> *find k points within the feature space that is closest to our point*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Voronoi Tesselation\n",
    "A Voronoi Diagram denotes partitioning of points (also called as seeds, sites or generators) into regions such that the region holds all points that is closest to a particular point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems\n",
    "[[1]](#Problem-1) ***Titanic: Machine Learning from Disaster***: Predict one's survival on the Titanic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 1\n",
    "> ***Titanic: Machine Learning from Disaster***: Predict one's survival on the Titanic\n",
    "\n",
    "We begin moving towards our first Kaggle problem, somewhat a *\"Hello, World!\"* substitute for Data Science. Before even begining to articulate a well-defined problem statement, I suggest you try to flush out everything you know about the Titanic, including the movie. Forget all observations and inferences you made or can make if you heard the word - *Titanic*. Let's assume we're given no other information except the title stated above and a training and testing data set.\n",
    "\n",
    "***Q. \"Why? I don't get it.\"***\n",
    "\n",
    "Whenever you aim towards attacking any problem in Data Science, remind yourself that you're nothing more than just a Data Scientist or Machine Learning Engineer (in the making). Our expertise is limited within the boundaries of our domain and any kind of preconcieved notions or knowledge about our data set tends to restrict our observations towards that direction. Many a times, data sets exhibit relations that goes unnoticed by the domain it belongs to. And that is what data analysis is for. Below is a great talk on *Data Agnosticism: Feature Engineering Without Domain Expertise* by Nicolas Kridler.\n",
    "\n",
    "[![](http://img.youtube.com/vi/bL4b1sGnILU/0.jpg)](http://www.youtube.com/watch?v=bL4b1sGnILU)\n",
    "\n",
    "One of the main takeaway from the talk is that *let the machine make sense of what it takes*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load our data set into Pandas' DataFrame Objects. I've created a function called `load_titanic()` that returns a dictionary holding 2 data frames, one for our training set while the other for our testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_titanic():\n",
    "    path = {\n",
    "        'train': './data/titanic/train.csv',\n",
    "        'test' : './data/titanic/test.csv'\n",
    "    }\n",
    "    \n",
    "    df   = { \n",
    "        'train': pd.read_csv(path['train']),\n",
    "        'test' : pd.read_csv(path['test'])\n",
    "    }\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's view our training set for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## References\n",
    "[[1]](http://page.mi.fu-berlin.de/prechelt/Biblio/jccpprt_computer2000.pdf) Prechelt, Lutz. *An Empirical Comparison of C, C , Java, Perl, Python, Rexx, and Tcl for a Search, String Processing Program.* Karlsruhe: U, Fak. Für Informatik, 2000.\n",
    "\n",
    "[[2]](http://repository.cmu.edu/cgi/viewcontent.cgi?article=2874&context=compsci) Pomerleau, Dean A. *ALVINN, an Autonomous Land Vehicle in a Neural Network.* Pittsburgh, PA: Carnegie Mellon U, Computer Science Dept., 1989.\n",
    "\n",
    "[[3]](http://www.theverge.com/2016/3/24/11297050/tay-microsoft-chatbot-racist) Twitter taught Microsoft’s AI chatbot to be a racist asshole in less than a day - The Verge\n",
    "\n",
    "[[4]](http://www.independent.co.uk/life-style/gadgets-and-tech/news/stephen-hawking-artificial-intelligence-could-wipe-out-humanity-when-it-gets-too-clever-as-humans-a6686496.html) Stephen Hawking: Artificial intelligence could wipe out humanity when it gets too clever as humans will be like ants\n",
    "\n",
    "[[5]](https://archive.ics.uci.edu/ml/datasets/Housing) UCI Machine Learning Repository - Housing Data Set\n",
    "\n",
    "\n",
    "\n",
    "[[6]](http://cs224d.stanford.edu/lecture_notes/LectureNotes3.pdf) Lecture Notes 3 on *Deep Learning for NLP* - CS Stanford \n",
    "\n",
    "[[7]](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf) Lecun, Yann, Leon Bottou, Genevieve B. Orr, and Klaus Robert Müller. *Efficient BackProp.* 1998\n",
    "\n",
    "[[8]](http://stats.stackexchange.com/questions/115258/comprehensive-list-of-activation-functions-in-neural-networks-with-pros-cons) Comprehensive list of activation functions in neural networks with pros/cons\n",
    "\n",
    "[[9]](http://stackoverflow.com/questions/20027598/why-should-weights-of-neural-networks-be-initialized-to-random-numbers) Why should weights of Neural Networks be initialized to random numbers?\n",
    "\n",
    "[[10]](http://stats.stackexchange.com/questions/154879/a-list-of-cost-functions-used-in-neural-networks-alongside-applications) A list of cost functions used in neural networks, alongside applications\n",
    "\n",
    "[[11]](https://www.youtube.com/watch?v=56TYLaQN4N8) YouTube Lecture on *Deep Learning Lecture 12: Recurrent Neural Nets and LSTMs* by Nando de Freitas\n",
    "\n",
    "[[12]](http://www.comp.dit.ie/btierney/BSI/Han%20Book%20Ch3%20DataExploration.pdf) Data Preprocessing\n",
    "\n",
    "[[13]](http://krishikosh.egranth.ac.in/bitstream/1/64144/1/CCSHAU-Harish%20S.pdf) Varietal Identification And Seed Vigour Assessment In Cotton - Dr. Harish S."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
